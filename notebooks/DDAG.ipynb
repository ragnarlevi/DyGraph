{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys,os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_t, norm, multivariate_normal\n",
    "import sklearn\n",
    "from sklearn.covariance import GraphicalLasso\n",
    "import scipy\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "import time\n",
    "import tqdm\n",
    "import networkx as nx\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import expm\n",
    "\n",
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 25,\n",
    "         'axes.labelsize': 25,\n",
    "         'axes.titlesize':25,\n",
    "         'xtick.labelsize':25,\n",
    "         'ytick.labelsize':'x-large',\n",
    "          'axes.titlesize' : 'x-large'}\n",
    "pylab.rcParams.update(params)\n",
    "sys.path.insert(0, 'C:/Users/User/Code/DyGraph')\n",
    "\n",
    "import DyGraph as dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "import igraph as ig\n",
    "import random\n",
    "\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def is_dag(W):\n",
    "    G = ig.Graph.Weighted_Adjacency(W.tolist())\n",
    "    return G.is_dag()\n",
    "\n",
    "\n",
    "def simulate_dag(d, s0, graph_type):\n",
    "    \"\"\"Simulate random DAG with some expected number of edges.\n",
    "\n",
    "    Args:\n",
    "        d (int): num of nodes\n",
    "        s0 (int): expected num of edges\n",
    "        graph_type (str): ER, SF, BP\n",
    "\n",
    "    Returns:\n",
    "        B (np.ndarray): [d, d] binary adj matrix of DAG\n",
    "    \"\"\"\n",
    "    def _random_permutation(M):\n",
    "        # np.random.permutation permutes first axis only\n",
    "        P = np.random.permutation(np.eye(M.shape[0]))\n",
    "        return P.T @ M @ P\n",
    "\n",
    "    def _random_acyclic_orientation(B_und):\n",
    "        return np.tril(_random_permutation(B_und), k=-1)\n",
    "\n",
    "    def _graph_to_adjmat(G):\n",
    "        return np.array(G.get_adjacency().data)\n",
    "\n",
    "    if graph_type == 'ER':\n",
    "        # Erdos-Renyi\n",
    "        G_und = ig.Graph.Erdos_Renyi(n=d, m=s0)\n",
    "        B_und = _graph_to_adjmat(G_und)\n",
    "        B = _random_acyclic_orientation(B_und)\n",
    "    elif graph_type == 'SF':\n",
    "        # Scale-free, Barabasi-Albert\n",
    "        G = ig.Graph.Barabasi(n=d, m=int(round(s0 / d)), directed=True)\n",
    "        B = _graph_to_adjmat(G)\n",
    "    elif graph_type == 'BP':\n",
    "        # Bipartite, Sec 4.1 of (Gu, Fu, Zhou, 2018)\n",
    "        top = int(0.2 * d)\n",
    "        G = ig.Graph.Random_Bipartite(top, d - top, m=s0, directed=True, neimode=ig.OUT)\n",
    "        B = _graph_to_adjmat(G)\n",
    "    else:\n",
    "        raise ValueError('unknown graph type')\n",
    "    B_perm = _random_permutation(B)\n",
    "    assert ig.Graph.Adjacency(B_perm.tolist()).is_dag()\n",
    "    return B_perm\n",
    "\n",
    "\n",
    "def simulate_parameter(B, w_ranges=((-2.0, -0.5), (0.5, 2.0))):\n",
    "    \"\"\"Simulate SEM parameters for a DAG.\n",
    "\n",
    "    Args:\n",
    "        B (np.ndarray): [d, d] binary adj matrix of DAG\n",
    "        w_ranges (tuple): disjoint weight ranges\n",
    "\n",
    "    Returns:\n",
    "        W (np.ndarray): [d, d] weighted adj matrix of DAG\n",
    "    \"\"\"\n",
    "    W = np.zeros(B.shape)\n",
    "    S = np.random.randint(len(w_ranges), size=B.shape)  # which range\n",
    "    for i, (low, high) in enumerate(w_ranges):\n",
    "        U = np.random.uniform(low=low, high=high, size=B.shape)\n",
    "        W += B * (S == i) * U\n",
    "    return W\n",
    "\n",
    "\n",
    "def simulate_linear_sem(W, n, sem_type, noise_scale=None):\n",
    "    \"\"\"Simulate samples from linear SEM with specified type of noise.\n",
    "\n",
    "    For uniform, noise z ~ uniform(-a, a), where a = noise_scale.\n",
    "\n",
    "    Args:\n",
    "        W (np.ndarray): [d, d] weighted adj matrix of DAG\n",
    "        n (int): num of samples, n=inf mimics population risk\n",
    "        sem_type (str): gauss, exp, gumbel, uniform, logistic, poisson\n",
    "        noise_scale (np.ndarray): scale parameter of additive noise, default all ones\n",
    "\n",
    "    Returns:\n",
    "        X (np.ndarray): [n, d] sample matrix, [d, d] if n=inf\n",
    "    \"\"\"\n",
    "    def _simulate_single_equation(X, w, scale):\n",
    "        \"\"\"X: [n, num of parents], w: [num of parents], x: [n]\"\"\"\n",
    "        if sem_type == 'gauss':\n",
    "            z = np.random.normal(scale=scale, size=n)\n",
    "            x = X @ w + z\n",
    "        elif sem_type == 'exp':\n",
    "            z = np.random.exponential(scale=scale, size=n)\n",
    "            x = X @ w + z\n",
    "        elif sem_type == 'gumbel':\n",
    "            z = np.random.gumbel(scale=scale, size=n)\n",
    "            x = X @ w + z\n",
    "        elif sem_type == 'uniform':\n",
    "            z = np.random.uniform(low=-scale, high=scale, size=n)\n",
    "            x = X @ w + z\n",
    "        elif sem_type == 'logistic':\n",
    "            x = np.random.binomial(1, sigmoid(X @ w)) * 1.0\n",
    "        elif sem_type == 'poisson':\n",
    "            x = np.random.poisson(np.exp(X @ w)) * 1.0\n",
    "        else:\n",
    "            raise ValueError('unknown sem type')\n",
    "        return x\n",
    "\n",
    "    d = W.shape[0]\n",
    "    if noise_scale is None:\n",
    "        scale_vec = np.ones(d)\n",
    "    elif np.isscalar(noise_scale):\n",
    "        scale_vec = noise_scale * np.ones(d)\n",
    "    else:\n",
    "        if len(noise_scale) != d:\n",
    "            raise ValueError('noise scale must be a scalar or has length d')\n",
    "        scale_vec = noise_scale\n",
    "    if not is_dag(W):\n",
    "        raise ValueError('W must be a DAG')\n",
    "    if np.isinf(n):  # population risk for linear gauss SEM\n",
    "        if sem_type == 'gauss':\n",
    "            # make 1/d X'X = true cov\n",
    "            X = np.sqrt(d) * np.diag(scale_vec) @ np.linalg.inv(np.eye(d) - W)\n",
    "            return X\n",
    "        else:\n",
    "            raise ValueError('population risk not available')\n",
    "    # empirical risk\n",
    "    G = ig.Graph.Weighted_Adjacency(W.tolist())\n",
    "    ordered_vertices = G.topological_sorting()\n",
    "    assert len(ordered_vertices) == d\n",
    "    X = np.zeros([n, d])\n",
    "    for j in ordered_vertices:\n",
    "        parents = G.neighbors(j, mode=ig.IN)\n",
    "        X[:, j] = _simulate_single_equation(X[:, parents], W[parents, j], scale_vec[j])\n",
    "    return X\n",
    "\n",
    "\n",
    "def simulate_nonlinear_sem(B, n, sem_type, noise_scale=None):\n",
    "    \"\"\"Simulate samples from nonlinear SEM.\n",
    "\n",
    "    Args:\n",
    "        B (np.ndarray): [d, d] binary adj matrix of DAG\n",
    "        n (int): num of samples\n",
    "        sem_type (str): mlp, mim, gp, gp-add\n",
    "        noise_scale (np.ndarray): scale parameter of additive noise, default all ones\n",
    "\n",
    "    Returns:\n",
    "        X (np.ndarray): [n, d] sample matrix\n",
    "    \"\"\"\n",
    "    def _simulate_single_equation(X, scale):\n",
    "        \"\"\"X: [n, num of parents], x: [n]\"\"\"\n",
    "        z = np.random.normal(scale=scale, size=n)\n",
    "        pa_size = X.shape[1]\n",
    "        if pa_size == 0:\n",
    "            return z\n",
    "        if sem_type == 'mlp':\n",
    "            hidden = 100\n",
    "            W1 = np.random.uniform(low=0.5, high=2.0, size=[pa_size, hidden])\n",
    "            W1[np.random.rand(*W1.shape) < 0.5] *= -1\n",
    "            W2 = np.random.uniform(low=0.5, high=2.0, size=hidden)\n",
    "            W2[np.random.rand(hidden) < 0.5] *= -1\n",
    "            x = sigmoid(X @ W1) @ W2 + z\n",
    "        elif sem_type == 'mim':\n",
    "            w1 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
    "            w1[np.random.rand(pa_size) < 0.5] *= -1\n",
    "            w2 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
    "            w2[np.random.rand(pa_size) < 0.5] *= -1\n",
    "            w3 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
    "            w3[np.random.rand(pa_size) < 0.5] *= -1\n",
    "            x = np.tanh(X @ w1) + np.cos(X @ w2) + np.sin(X @ w3) + z\n",
    "        elif sem_type == 'gp':\n",
    "            from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "            gp = GaussianProcessRegressor()\n",
    "            x = gp.sample_y(X, random_state=None).flatten() + z\n",
    "        elif sem_type == 'gp-add':\n",
    "            from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "            gp = GaussianProcessRegressor()\n",
    "            x = sum([gp.sample_y(X[:, i, None], random_state=None).flatten()\n",
    "                     for i in range(X.shape[1])]) + z\n",
    "        else:\n",
    "            raise ValueError('unknown sem type')\n",
    "        return x\n",
    "\n",
    "    d = B.shape[0]\n",
    "    scale_vec = noise_scale if noise_scale else np.ones(d)\n",
    "    X = np.zeros([n, d])\n",
    "    G = ig.Graph.Adjacency(B.tolist())\n",
    "    ordered_vertices = G.topological_sorting()\n",
    "    assert len(ordered_vertices) == d\n",
    "    for j in ordered_vertices:\n",
    "        parents = G.neighbors(j, mode=ig.IN)\n",
    "        X[:, j] = _simulate_single_equation(X[:, parents], scale_vec[j])\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as slin\n",
    "import scipy.optimize as sopt\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "\n",
    "def notears_linear(X, lambda1, loss_type, max_iter=100, h_tol=1e-8, rho_max=1e+16, w_threshold=0.3):\n",
    "    \"\"\"Solve min_W L(W; X) + lambda1 ‖W‖_1 s.t. h(W) = 0 using augmented Lagrangian.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): [n, d] sample matrix\n",
    "        lambda1 (float): l1 penalty parameter\n",
    "        loss_type (str): l2, logistic, poisson\n",
    "        max_iter (int): max num of dual ascent steps\n",
    "        h_tol (float): exit if |h(w_est)| <= htol\n",
    "        rho_max (float): exit if rho >= rho_max\n",
    "        w_threshold (float): drop edge if |weight| < threshold\n",
    "\n",
    "    Returns:\n",
    "        W_est (np.ndarray): [d, d] estimated DAG\n",
    "    \"\"\"\n",
    "    def _loss(W):\n",
    "        \"\"\"Evaluate value and gradient of loss.\"\"\"\n",
    "        M = X @ W\n",
    "        if loss_type == 'l2':\n",
    "            R = X - M\n",
    "            loss = 0.5 / X.shape[0] * (R ** 2).sum()\n",
    "            G_loss = - 1.0 / X.shape[0] * X.T @ R\n",
    "        elif loss_type == 'logistic':\n",
    "            loss = 1.0 / X.shape[0] * (np.logaddexp(0, M) - X * M).sum()\n",
    "            G_loss = 1.0 / X.shape[0] * X.T @ (sigmoid(M) - X)\n",
    "        elif loss_type == 'poisson':\n",
    "            S = np.exp(M)\n",
    "            loss = 1.0 / X.shape[0] * (S - X * M).sum()\n",
    "            G_loss = 1.0 / X.shape[0] * X.T @ (S - X)\n",
    "        else:\n",
    "            raise ValueError('unknown loss type')\n",
    "        return loss, G_loss\n",
    "\n",
    "    def _h(W):\n",
    "        \"\"\"Evaluate value and gradient of acyclicity constraint.\"\"\"\n",
    "        E = slin.expm(W * W)  # (Zheng et al. 2018)\n",
    "        h = np.trace(E) - d\n",
    "        #     # A different formulation, slightly faster at the cost of numerical stability\n",
    "        #     M = np.eye(d) + W * W / d  # (Yu et al. 2019)\n",
    "        #     E = np.linalg.matrix_power(M, d - 1)\n",
    "        #     h = (E.T * M).sum() - d\n",
    "        G_h = E.T * W * 2\n",
    "        return h, G_h\n",
    "\n",
    "    def _adj(w):\n",
    "        \"\"\"Convert doubled variables ([2 d^2] array) back to original variables ([d, d] matrix).\"\"\"\n",
    "        return (w[:d * d] - w[d * d:]).reshape([d, d])\n",
    "\n",
    "    def _func(w):\n",
    "        \"\"\"Evaluate value and gradient of augmented Lagrangian for doubled variables ([2 d^2] array).\"\"\"\n",
    "        W = _adj(w)\n",
    "        loss, G_loss = _loss(W)\n",
    "        h, G_h = _h(W)\n",
    "        obj = loss + 0.5 * rho * h * h + alpha * h + lambda1 * w.sum()\n",
    "        G_smooth = G_loss + (rho * h + alpha) * G_h\n",
    "        g_obj = np.concatenate((G_smooth + lambda1, - G_smooth + lambda1), axis=None)\n",
    "        return obj, g_obj\n",
    "\n",
    "    n, d = X.shape\n",
    "    w_est, rho, alpha, h = np.zeros(2 * d * d), 1.0, 0.0, np.inf  # double w_est into (w_pos, w_neg)\n",
    "    bnds = [(0, 0) if i == j else (0, None) for _ in range(2) for i in range(d) for j in range(d)]\n",
    "    if loss_type == 'l2':\n",
    "        X = X - np.mean(X, axis=0, keepdims=True)\n",
    "    for _ in range(max_iter):\n",
    "        w_new, h_new = None, None\n",
    "        while rho < rho_max:\n",
    "            sol = sopt.minimize(_func, w_est, method='L-BFGS-B', jac=True, bounds=bnds)\n",
    "            w_new = sol.x\n",
    "            h_new, _ = _h(_adj(w_new))\n",
    "            if h_new > 0.25 * h:\n",
    "                rho *= 10\n",
    "            else:\n",
    "                break\n",
    "        w_est, h = w_new, h_new\n",
    "        alpha += rho * h\n",
    "        if h <= h_tol or rho >= rho_max:\n",
    "            break\n",
    "    W_est = _adj(w_est)\n",
    "    W_est[np.abs(W_est) < w_threshold] = 0\n",
    "    return W_est\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def soft_threshold_odd( A, lamda):\n",
    "\n",
    "    \"\"\"\n",
    "    diagonal lasso penalty\n",
    "\n",
    "    Parameters\n",
    "    ------------------\n",
    "    A: np.array,\n",
    "    \n",
    "    lamda: float,\n",
    "        regularization\n",
    "    \"\"\"\n",
    "    opt_m = (A-lamda)*(A>=lamda) + (A+lamda)*(A<=-lamda)\n",
    "    \n",
    "\n",
    "    return opt_m\n",
    "\n",
    "\n",
    "\n",
    "def _h(W):\n",
    "    \"\"\"Evaluate value and gradient of acyclicity constraint.\"\"\"\n",
    "    E = expm(W * W)  # (Zheng et al. 2018)\n",
    "    h = np.trace(E) - W.shape[0]\n",
    "    #     # A different formulation, slightly faster at the cost of numerical stability\n",
    "    #     M = np.eye(d) + W * W / d  # (Yu et al. 2019)\n",
    "    #     E = np.linalg.matrix_power(M, d - 1)\n",
    "    #     h = (E.T * M).sum() - d\n",
    "    G_h = E.T * W * 2\n",
    "    return h, G_h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate DAG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.895758831183038"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_per_graph = 100\n",
    "d = 5\n",
    "X = np.zeros((obs_per_graph, d))\n",
    "sigma = 0.1\n",
    "rnd = np.random.RandomState(42)\n",
    "\n",
    "n, s0, graph_type, sem_type = 1000,  5, 'ER', 'gauss'\n",
    "I = np.identity(d)\n",
    "B_true = simulate_dag(d, s0, graph_type)\n",
    "A = simulate_parameter(B_true)\n",
    "\n",
    "W = np.linalg.inv(I-A)\n",
    "for i in range(obs_per_graph):\n",
    "\n",
    "\n",
    "    X[i] = np.dot(W, np.random.normal(0,1,5))\n",
    "\n",
    "\n",
    "I = np.identity(X.shape[1])\n",
    "np.linalg.cond(W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "G = nx.from_numpy_array(A,create_using = nx.DiGraph)\n",
    "nx.draw(G)\n",
    "nx.is_directed_acyclic_graph(G)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NO tears"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Paper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.83485807  0.          0.        ]\n",
      " [-1.82010469  0.          0.          1.91830353  0.        ]\n",
      " [-1.33993634  0.          0.          0.          1.69273396]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n",
      "[[ 0.   -0.   -0.   -0.    0.  ]\n",
      " [ 0.    0.    0.86  0.    0.  ]\n",
      " [-1.62  0.    0.    2.04  0.  ]\n",
      " [-1.33  0.    0.    0.    1.49]\n",
      " [ 0.    0.    0.    0.    0.  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set_random_seed(1)\n",
    "\n",
    "print(A)\n",
    "\n",
    "A_est = notears_linear(X, lambda1=0.1, loss_type='l2', w_threshold=0)\n",
    "print(np.round(A_est.T,2))\n",
    "A_est[np.abs(A_est)<1e-2] = 0\n",
    "G = nx.from_numpy_array(A_est,create_using = nx.DiGraph)\n",
    "nx.is_directed_acyclic_graph(G)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My ADMM implementation of l2 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   -0.   -0.   -0.    0.  ]\n",
      " [-0.    0.    0.86  0.   -0.  ]\n",
      " [-1.62  0.    0.    2.04 -0.  ]\n",
      " [-1.33  0.    0.    0.    1.49]\n",
      " [ 0.    0.    0.    0.    0.  ]]\n",
      " is dag True\n",
      "\n",
      "\n",
      "[[ 0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.83  0.    0.  ]\n",
      " [-1.82  0.    0.    1.92  0.  ]\n",
      " [-1.34  0.    0.    0.    1.69]\n",
      " [ 0.    0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def _loss(W, X):\n",
    "    \"\"\"Evaluate value and gradient of loss.\"\"\"\n",
    "    M = X @ W\n",
    "\n",
    "    R = X - M\n",
    "    loss = 0.5 / X.shape[0] * (R ** 2).sum()\n",
    "    G_loss = - 1.0 / X.shape[0] * X.T @ R\n",
    "    return loss, G_loss\n",
    "\n",
    "\n",
    "\n",
    "# def l2_loss(param,a,u,rho,kappa, X):\n",
    "#     param = np.reshape(param,(d,d))\n",
    "#     M = X @ param\n",
    "#     h,g_h = _h(param)\n",
    "#     loss, G_loss = _loss(param, X)\n",
    "\n",
    "#     R = X - M\n",
    "#     obj = loss +  0.5*kappa*h**2  + 0.5*rho*scipy.linalg.norm(a - param + u, ord = 'fro')**2  \n",
    "#     G_loss = (G_loss + kappa*h*g_h - rho*(a - param + u)).flatten()\n",
    "#     return obj, G_loss\n",
    "\n",
    "\n",
    "def l2_loss(param,a,u,rho,kappa, X):\n",
    "    param = np.reshape(param,(d,d))\n",
    "    M = X @ param\n",
    "    h,g_h = _h(param)\n",
    "\n",
    "    R = X - M\n",
    "    loss = 0.5 / X.shape[0] * (R ** 2).sum() +  0.5*kappa*h**2  + 0.5*rho*scipy.linalg.norm(param - a + u, ord = 'fro')**2  \n",
    "    G_loss = (- 1.0 / X.shape[0] * X.T @ R + kappa*h*g_h + rho*(param - a + u)).flatten()\n",
    "    return loss, G_loss\n",
    "\n",
    "u0 = np.array([ np.zeros((d,d)) for  i in range(1)]) # np.array([gen_low_tri() for  i in range(1)])\n",
    "z0 = np.array([ np.zeros((d,d)) for  i in range(1)]) # np.array([gen_low_tri() for  i in range(1)])\n",
    "np.fill_diagonal(z0[0], 0)\n",
    "z1 =  np.array([np.zeros((d,d)) for  i in range(1)]) # np.array([gen_low_tri() for  i in range(1)])\n",
    "np.fill_diagonal(z1[0], 0)\n",
    "theta = np.array([np.zeros((d,d)) for i in range(1) ]) # np.random.uniform(-a,a, size=(d,d))\n",
    "np.fill_diagonal(theta[0], 0)\n",
    "\n",
    "X_c = X - np.mean(X, axis=0, keepdims=True)\n",
    "kappa_max = 1e10\n",
    "lamda = 0.1\n",
    "alpha = 0\n",
    "rho = 1\n",
    "kappa = 1\n",
    "\n",
    "z0_pre = z0[0]\n",
    "theta_pre = theta[0]\n",
    "u0_pre = u0[0]\n",
    "h = np.inf\n",
    "for _ in range(10):\n",
    "    if kappa >= kappa_max:\n",
    "        break\n",
    "    w_new, h_new = None, None\n",
    "    while kappa < kappa_max:\n",
    "\n",
    "        z0[0] = z0_pre\n",
    "        theta[0] = theta_pre\n",
    "        u0[0] = u0_pre\n",
    "        \n",
    "        for admm_idx in range(5):\n",
    "            start = time.time()\n",
    "            out = minimize(l2_loss, theta[0].flatten(), args = (z0[0], u0[0], rho, kappa, X_c), jac=True, method = 'L-BFGS-B')\n",
    "            theta[0] = np.reshape(out.x, (d,d))\n",
    "            np.fill_diagonal(theta[0],0)\n",
    "\n",
    "            z0[0] = soft_threshold_odd(theta[0] + u0[0]  , lamda/rho) #+ z1[0]-u1[0]\n",
    "            np.fill_diagonal(z0[0],0)\n",
    "\n",
    "            u0[0] = u0[0] + theta[0] - z0[0] \n",
    "\n",
    "        h_new, _ = _h(theta[0])\n",
    "        if h_new > 0.25 * h:\n",
    "            kappa *= 10\n",
    "            # break\n",
    "        else:\n",
    "            break\n",
    "    h = h_new\n",
    "    z0_pre = z0[0]\n",
    "    theta_pre = theta[0]\n",
    "    u0_pre = u0[0]\n",
    "\n",
    "print(np.round(theta[0].T,2))\n",
    "theta[0][np.abs(theta[0])<1e-2] = 0\n",
    "G = nx.from_numpy_array(theta[0].T,create_using = nx.DiGraph)\n",
    "print(f\" is dag {nx.is_directed_acyclic_graph(G)}\")\n",
    "print(\"\\n\")\n",
    "print(np.round(A,2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My implementation of Gaussian loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   -0.   -0.    0.    0.01]\n",
      " [-0.7   0.    0.31  1.25 -0.06]\n",
      " [-1.16  0.    0.    2.09  0.11]\n",
      " [-0.71  0.    0.    0.    0.92]\n",
      " [ 0.05  0.    0.    0.    0.  ]]\n",
      " is dag True\n",
      "\n",
      "\n",
      "[[ 0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.83  0.    0.  ]\n",
      " [-1.82  0.    0.    1.92  0.  ]\n",
      " [-1.34  0.    0.    0.    1.69]\n",
      " [ 0.    0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def gauss_loss(param,a,rho,kappa, S):\n",
    "    A = np.reshape(param,(d,d))\n",
    "    h,g_h = _h(A)\n",
    "    I = np.identity(d)\n",
    "\n",
    "    IA = I-A\n",
    "    v,_ = np.linalg.eig(IA)\n",
    "    gauss_loss = -2*np.sum(np.log(v[v>0])) + 0.5*np.trace(np.dot(IA,IA.T).dot(S))\n",
    "    g_gauss = 2*np.linalg.inv(IA+0.01*I).T - 2*np.dot(S,IA)\n",
    "    loss = gauss_loss +  0.5*kappa*h**2  + 0.5*rho*scipy.linalg.norm(A - a, ord = 'fro')**2  \n",
    "    G_loss = (g_gauss + kappa*h*g_h + rho*(A - a )).flatten()\n",
    "    return loss, G_loss\n",
    "\n",
    "\n",
    "z0 = np.array([ np.zeros((d,d)) for  i in range(1)]) # np.array([gen_low_tri() for  i in range(1)])\n",
    "np.fill_diagonal(z0[0], 0)\n",
    "theta = np.array([np.zeros((d,d)) for i in range(1) ]) # np.random.uniform(-a,a, size=(d,d))\n",
    "np.fill_diagonal(theta[0], 0)\n",
    "u0 = np.array([ np.zeros((d,d)) for  i in range(1)])\n",
    "\n",
    "X_c = X#  - np.mean(X, axis=0, keepdims=True)\n",
    "n, d = X_c.shape\n",
    "S = np.cov(X_c.T)*(n-1)/n\n",
    "kappa_max = 1e10\n",
    "lamda = 0.1\n",
    "alpha = 0\n",
    "rho = 10\n",
    "kappa = 1\n",
    "\n",
    "z0_pre = z0[0]\n",
    "theta_pre = theta[0]\n",
    "u0_pre = u0[0]\n",
    "h = np.inf\n",
    "for _ in range(10):\n",
    "    if kappa >= kappa_max:\n",
    "        break\n",
    "    w_new, h_new = None, None\n",
    "    while kappa < kappa_max:\n",
    "\n",
    "        z0[0] = z0_pre\n",
    "        theta[0] = theta_pre\n",
    "        u0[0] = u0_pre\n",
    "        \n",
    "        for admm_idx in range(10):\n",
    "            start = time.time()\n",
    "            out = minimize(gauss_loss, theta[0].flatten(), args = (z0[0]- u0[0], rho, kappa, S), jac=True, method = 'L-BFGS-B')\n",
    "            theta[0] = np.reshape(out.x, (d,d))\n",
    "            np.fill_diagonal(theta[0],0)\n",
    "\n",
    "            z0[0] = soft_threshold_odd(theta[0] + u0[0]  , lamda/rho) #+ z1[0]-u1[0]\n",
    "            np.fill_diagonal(z0[0],0)\n",
    "\n",
    "            u0[0] = u0[0] + theta[0] - z0[0] \n",
    "\n",
    "        h_new, _ = _h(theta[0])\n",
    "        if h_new > 0.25 * h:\n",
    "            kappa *= 10\n",
    "        else:\n",
    "            break\n",
    "    h = h_new\n",
    "    z0_pre = z0[0]\n",
    "    theta_pre = theta[0]\n",
    "    u0_pre = u0[0]\n",
    "\n",
    "\n",
    "print(np.round(theta[0].T,2))\n",
    "theta[0][np.abs(theta[0])<1e-1] = 0\n",
    "G = nx.from_numpy_array(theta[0].T,create_using = nx.DiGraph)\n",
    "print(f\" is dag {nx.is_directed_acyclic_graph(G)}\")\n",
    "print(\"\\n\")\n",
    "print(np.round(A,2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work, but Gaussian is more noisy. Might be because of log det and inversion. Will have a look. Have to apply a threshold to, make it a DAG."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1  = np. array([[ 0.        , -1.89159833,  0.        ,  1.09628855,  0.        ],\n",
    "       [ 0.        ,  0.        , -0.9,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        , -1,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  1.44408539,  0.        ,  0.        ]])\n",
    "\n",
    "A2 = A1.copy()\n",
    "A2[0,3] = 0\n",
    "\n",
    "A3 = A2.copy()\n",
    "\n",
    "A = [A1, A2, A3]\n",
    "X_dynamic = []\n",
    "obs_per_graph = 100\n",
    "for t in range(3):\n",
    "    x_tmp = np.zeros((obs_per_graph, d))\n",
    "    I = np.identity(d)\n",
    "    W_tmp = np.linalg.inv(I-A[t])\n",
    "    for i in range(obs_per_graph):\n",
    "\n",
    "        x_tmp[i] = np.dot(W_tmp, np.random.normal(0,1,5))\n",
    "\n",
    "    X_dynamic.append(x_tmp)\n",
    "\n",
    "X_dynamic = np.vstack(X_dynamic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  , -1.97, -0.32,  0.82, -0.04],\n",
       "       [-0.  ,  0.  , -1.03,  0.02,  0.05],\n",
       "       [ 0.  , -0.  ,  0.  , -0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  , -1.06,  0.  , -0.06],\n",
       "       [-0.  ,  0.  ,  1.51, -0.  ,  0.  ]])"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_est = notears_linear(X_dynamic[:100], lambda1=0.01, loss_type='l2', w_threshold=0)\n",
    "np.round(A_est.T,2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l2 ADMM temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   -1.87  0.    0.81 -0.09]\n",
      " [-0.    0.   -0.73  0.01 -0.08]\n",
      " [ 0.   -0.    0.   -0.    0.  ]\n",
      " [ 0.    0.01 -0.84  0.   -0.09]\n",
      " [-0.   -0.    1.4  -0.    0.  ]]\n",
      " is dag True\n",
      "\n",
      "\n",
      "[[ 0.   -1.87  0.08  0.05 -0.  ]\n",
      " [-0.    0.   -0.74  0.02 -0.09]\n",
      " [ 0.   -0.    0.   -0.    0.  ]\n",
      " [ 0.01  0.01 -0.84  0.   -0.07]\n",
      " [ 0.   -0.    1.4  -0.    0.  ]]\n",
      " is dag True\n",
      "\n",
      "\n",
      "[[ 0.   -1.63  0.08  0.    0.04]\n",
      " [-0.    0.   -0.54  0.02 -0.09]\n",
      " [ 0.   -0.    0.   -0.    0.  ]\n",
      " [-0.    0.01 -0.67  0.   -0.07]\n",
      " [ 0.01 -0.    1.08 -0.    0.  ]]\n",
      " is dag True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def _loss(W, X):\n",
    "    \"\"\"Evaluate value and gradient of loss.\"\"\"\n",
    "    M = X @ W\n",
    "\n",
    "    R = X - M\n",
    "    loss = 0.5 / X.shape[0] * (R ** 2).sum()\n",
    "    G_loss = - 1.0 / X.shape[0] * X.T @ R\n",
    "    return loss, G_loss\n",
    "\n",
    "\n",
    "def l2_loss(param,a,rho,kappa, X):\n",
    "    param = np.reshape(param,(d,d))\n",
    "    M = X @ param\n",
    "    h,g_h = _h(param)\n",
    "\n",
    "    R = X - M\n",
    "    loss = 0.5 / X.shape[0] * (R ** 2).sum() +  0.5*kappa*h**2  + 0.5*rho*scipy.linalg.norm(param - a, ord = 'fro')**2  \n",
    "    G_loss = (- 1.0 / X.shape[0] * X.T @ R + kappa*h*g_h + rho*(param - a)).flatten()\n",
    "    return loss, G_loss\n",
    "\n",
    "T = 3\n",
    "z0 = np.array([ np.zeros((d,d)) for  i in range(T)]) # np.array([gen_low_tri() for  i in range(1)])\n",
    "z1 =  np.array([np.zeros((d,d)) for  i in range(T)]) # np.array([gen_low_tri() for  i in range(1)])\n",
    "z2 =  np.array([np.zeros((d,d)) for  i in range(T)]) # np.array([gen_low_tri() for  i in range(1)])\n",
    "\n",
    "u0 = np.array([ np.zeros((d,d)) for  i in range(T)]) # np.array([gen_low_tri() for  i in range(1)])\n",
    "u1 =  np.array([np.zeros((d,d)) for  i in range(T)]) # np.array([gen_low_tri() for  i in range(1)])\n",
    "u2 =  np.array([np.zeros((d,d)) for  i in range(T)]) # np.array([gen_low_tri() for  i in range(1)])\n",
    "\n",
    "theta = np.array([np.zeros((d,d)) for i in range(T) ]) # np.random.uniform(-a,a, size=(d,d))\n",
    "np.fill_diagonal(theta[0], 0)\n",
    "\n",
    "X_c = X_dynamic - np.mean(X_dynamic, axis=0, keepdims=True)\n",
    "kappa_max = 1e10\n",
    "lamda = 0.1\n",
    "alpha = 0.3\n",
    "rho = 1\n",
    "kappa = 1\n",
    "\n",
    "z0_pre = z0\n",
    "theta_pre = theta\n",
    "u0_pre = u0\n",
    "h = np.inf\n",
    "for _ in range(10):\n",
    "    if kappa >= kappa_max:\n",
    "        break\n",
    "    w_new, h_new = None, None\n",
    "    while kappa < kappa_max:\n",
    "\n",
    "        z0 = z0_pre\n",
    "        theta = theta_pre\n",
    "        u0 = u0_pre\n",
    "        u1_pre = u1\n",
    "        u2_pre = u2\n",
    "        \n",
    "        for admm_idx in range(5):\n",
    "            for t in range(T):\n",
    "                if t == 0 or t == T:\n",
    "                    A_tmp = (z0[t] + z1[t] + z2[t] - u0[t] - u1[t] - u2[t])/2.0\n",
    "                else:\n",
    "                    A_tmp = (z0[t] + z1[t] + z2[t] - u0[t] - u1[t] - u2[t])/3.0\n",
    "                \n",
    "                out = minimize(l2_loss, theta[t].flatten(), args = (A_tmp, rho, kappa, X_c[t*obs_per_graph:(t+1)*obs_per_graph]), jac=True, method = 'L-BFGS-B')\n",
    "                theta[t] = np.reshape(out.x, (d,d))\n",
    "                np.fill_diagonal(theta[t],0)\n",
    "\n",
    "            start = time.time()\n",
    "            for t in range(T):\n",
    "                z0[t] = soft_threshold_odd(theta[t] + u0[t], lamda/rho) #+ z1[0]-u1[0]\n",
    "                np.fill_diagonal(z0[t],0)\n",
    "\n",
    "            for t in range(1,T):\n",
    "                A_star = theta[t]-theta[t-1]+u2[t]-u1[t-1]\n",
    "\n",
    "                E = soft_threshold_odd(A_star, 2*alpha/rho)\n",
    "                summ = 0.5*(theta[t]+theta[t-1]+u1[t-1]+u2[t])\n",
    "                z1[t-1] = summ - 0.5*E\n",
    "                z2[t] = summ + 0.5*E\n",
    "\n",
    "            u0 = u0 + theta - z0\n",
    "            u1[:(T-1)] = u1[:(T-1)] + theta[:(T-1)]-z1[:(T-1)]\n",
    "            u2[1:] = u2[1:] + theta[1:] - z2[1:]\n",
    "\n",
    "        h_new, _ = _h(theta[0])\n",
    "        if h_new > 0.25 * h:\n",
    "            kappa *= 10\n",
    "        else:\n",
    "            break\n",
    "    h = h_new\n",
    "    z0_pre = z0\n",
    "    theta_pre = theta\n",
    "    u0_pre = u0\n",
    "    u1_pre = u1\n",
    "    u2_pre = u2\n",
    "np.set_printoptions(suppress=True)\n",
    "print(np.round(theta[0].T,2))\n",
    "theta[0][np.abs(theta[0])<1e-1] = 0\n",
    "G = nx.from_numpy_array(theta[0].T,create_using = nx.DiGraph)\n",
    "print(f\" is dag {nx.is_directed_acyclic_graph(G)}\")\n",
    "print(\"\\n\")\n",
    "print(np.round(theta[1].T,2))\n",
    "theta[1][np.abs(theta[1])<1e-1] = 0\n",
    "G = nx.from_numpy_array(theta[1].T,create_using = nx.DiGraph)\n",
    "print(f\" is dag {nx.is_directed_acyclic_graph(G)}\")\n",
    "print(\"\\n\")\n",
    "print(np.round(theta[2].T,2))\n",
    "theta[2][np.abs(theta[2])<1e-1] = 0\n",
    "G = nx.from_numpy_array(theta[2].T,create_using = nx.DiGraph)\n",
    "print(f\" is dag {nx.is_directed_acyclic_graph(G)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.89159833,  0.        ,  1.09628855,  0.        ],\n",
       "       [ 0.        ,  0.        , -0.9       ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        , -1.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  1.44408539,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_dynamic[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   -1.75  0.06  0.66 -0.1 ]\n",
      " [-0.   -0.   -0.54  0.03 -0.13]\n",
      " [ 0.   -0.    0.   -0.    0.  ]\n",
      " [-0.   -0.03 -0.75  0.   -0.15]\n",
      " [-0.   -0.    1.25 -0.    0.  ]]\n",
      " is dag True\n",
      "\n",
      "\n",
      "[[ 0.   -1.82  0.07  0.01 -0.05]\n",
      " [-0.   -0.   -0.65 -0.   -0.1 ]\n",
      " [ 0.   -0.   -0.   -0.    0.  ]\n",
      " [ 0.09  0.25 -0.89 -0.    0.  ]\n",
      " [-0.   -0.    1.24 -0.08 -0.  ]]\n",
      " is dag True\n",
      "\n",
      "\n",
      "[[ 0.   -1.75  0.05 -0.    0.01]\n",
      " [-0.    0.   -0.54  0.01 -0.15]\n",
      " [ 0.    0.   -0.   -0.   -0.  ]\n",
      " [ 0.04  0.06 -0.79  0.   -0.05]\n",
      " [-0.   -0.01  1.16 -0.    0.  ]]\n",
      " is dag True\n"
     ]
    }
   ],
   "source": [
    "def gauss_loss(param,a,rho,kappa, S):\n",
    "    A = np.reshape(param,(d,d))\n",
    "    h,g_h = _h(A)\n",
    "    I = np.identity(d)\n",
    "\n",
    "    IA = I-A\n",
    "    v,_ = np.linalg.eig(IA)\n",
    "    gauss_loss = -2*np.sum(np.log(v[v>0])) + 0.5*np.trace(np.dot(IA,IA.T).dot(S))\n",
    "    g_gauss = np.linalg.inv(IA+0.1*I).T - np.dot(S,IA)\n",
    "    loss = gauss_loss +  0.5*kappa*h**2  + 0.5*rho*scipy.linalg.norm(A - a, ord = 'fro')**2  \n",
    "    G_loss = (g_gauss + kappa*h*g_h + rho*(A - a )).flatten()\n",
    "    return loss, G_loss\n",
    "\n",
    "\n",
    "T = 3\n",
    "z0 = np.array([ np.zeros((d,d)) for  i in range(T)]) # np.array([gen_low_tri() for  i in range(1)])\n",
    "z1 =  np.array([np.zeros((d,d)) for  i in range(T)]) # np.array([gen_low_tri() for  i in range(1)])\n",
    "z2 =  np.array([np.zeros((d,d)) for  i in range(T)]) # np.array([gen_low_tri() for  i in range(1)])\n",
    "\n",
    "u0 = np.array([ np.zeros((d,d)) for  i in range(T)]) # np.array([gen_low_tri() for  i in range(1)])\n",
    "u1 =  np.array([np.zeros((d,d)) for  i in range(T)]) # np.array([gen_low_tri() for  i in range(1)])\n",
    "u2 =  np.array([np.zeros((d,d)) for  i in range(T)]) # np.array([gen_low_tri() for  i in range(1)])\n",
    "\n",
    "theta = np.array([np.zeros((d,d)) for i in range(T) ]) # np.random.uniform(-a,a, size=(d,d))\n",
    "# np.fill_diagonal(theta[0], 0)\n",
    "\n",
    "X_c = X_dynamic - np.mean(X_dynamic, axis=0, keepdims=True)\n",
    "kappa_max = 1e10\n",
    "lamda = 0.1\n",
    "alpha = 0.8\n",
    "rho = 1\n",
    "kappa = 1\n",
    "\n",
    "z0_pre = z0\n",
    "z1_pre = z1\n",
    "z2_pre = z2\n",
    "theta_pre = theta\n",
    "u0_pre = u0\n",
    "h = np.inf\n",
    "for _ in range(10):\n",
    "    if kappa >= kappa_max:\n",
    "        break\n",
    "    w_new, h_new = None, None\n",
    "    while kappa < kappa_max:\n",
    "\n",
    "        z0 = z0_pre\n",
    "        z1 = z1_pre\n",
    "        z2 = z2_pre\n",
    "        theta = theta_pre\n",
    "        u0 = u0_pre\n",
    "        u1_pre = u1\n",
    "        u2_pre = u2\n",
    "        \n",
    "        for admm_idx in range(5):\n",
    "            for t in range(T):\n",
    "                if t == 0 or t == T:\n",
    "                    A_tmp = (z0[t] + z1[t] + z2[t] - u0[t] - u1[t] - u2[t])/2.0\n",
    "                else:\n",
    "                    A_tmp = (z0[t] + z1[t] + z2[t] - u0[t] - u1[t] - u2[t])/3.0\n",
    "                \n",
    "                S = np.cov(X_c[t*obs_per_graph:(t+1)*obs_per_graph].T)*(obs_per_graph-1)/obs_per_graph\n",
    "                out = minimize(gauss_loss, theta[t].flatten(), args = (A_tmp, rho, kappa, S), jac=True, method = 'L-BFGS-B')\n",
    "                theta[t] = np.reshape(out.x, (d,d))\n",
    "                #np.fill_diagonal(theta[t],0)\n",
    "\n",
    "            start = time.time()\n",
    "            for t in range(T):\n",
    "                z0[t] = soft_threshold_odd(theta[t] + u0[t], lamda/rho) #+ z1[0]-u1[0]\n",
    "                # np.fill_diagonal(z0[t],0)\n",
    "\n",
    "            for t in range(1,T):\n",
    "                A_star = theta[t]-theta[t-1]+u2[t]-u1[t-1]\n",
    "\n",
    "                E = soft_threshold_odd(A_star, 2*alpha/rho)\n",
    "                summ = 0.5*(theta[t]+theta[t-1]+u1[t-1]+u2[t])\n",
    "                z1[t-1] = summ - 0.5*E\n",
    "                z2[t] = summ + 0.5*E\n",
    "\n",
    "            u0 = u0 + theta - z0\n",
    "            u1[:(T-1)] = u1[:(T-1)] + theta[:(T-1)]-z1[:(T-1)]\n",
    "            u2[1:] = u2[1:] + theta[1:] - z2[1:]\n",
    "\n",
    "        h_new, _ = _h(theta[0])\n",
    "        if h_new > 0.25 * h:\n",
    "            kappa *= 10\n",
    "        else:\n",
    "            break\n",
    "    h = h_new\n",
    "    z0_pre = z0\n",
    "    z1_pre = z1\n",
    "    z2_pre = z2\n",
    "    theta_pre = theta\n",
    "    u0_pre = u0\n",
    "    u1_pre = u1\n",
    "    u2_pre = u2\n",
    "np.set_printoptions(suppress=True)\n",
    "print(np.round(theta[0].T,2))\n",
    "theta[0][np.abs(theta[0])<1e-1] = 0\n",
    "G = nx.from_numpy_array(theta[0].T,create_using = nx.DiGraph)\n",
    "print(f\" is dag {nx.is_directed_acyclic_graph(G)}\")\n",
    "print(\"\\n\")\n",
    "print(np.round(theta[1].T,2))\n",
    "theta[1][np.abs(theta[1])<1e-1] = 0\n",
    "G = nx.from_numpy_array(theta[1].T,create_using = nx.DiGraph)\n",
    "print(f\" is dag {nx.is_directed_acyclic_graph(G)}\")\n",
    "print(\"\\n\")\n",
    "print(np.round(theta[2].T,2))\n",
    "theta[2][np.abs(theta[2])<1e-1] = 0\n",
    "G = nx.from_numpy_array(theta[2].T,create_using = nx.DiGraph)\n",
    "print(f\" is dag {nx.is_directed_acyclic_graph(G)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DyGraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95fbe69513d588cbfb572e93f5d450d181f1d8d49af9eba3df43ef2e3376dba8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
